"""
Ragasç”¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
python src/ragas/create_testset.py
"""

from typing import List, Sequence, Iterable, Tuple
import csv
import glob
import os
from datetime import datetime
from pathlib import Path
from typing import Iterable, Sequence, Tuple

import openai
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from ragas.embeddings import OpenAIEmbeddings
from ragas.llms import LangchainLLMWrapper
from ragas.testset import TestsetGenerator
from ragas.testset.graph import KnowledgeGraph, Node, NodeType
from ragas.testset.persona import Persona
from ragas.testset.synthesizers.single_hop.specific import (
    SingleHopSpecificQuerySynthesizer,
)
from ragas.testset.transforms import (
    HeadlinesExtractor,
    HeadlineSplitter,
    KeyphrasesExtractor,
    apply_transforms,
)

from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

import config

load_dotenv()

project_root = config.PROJECT_ROOT
def build_knowledge_graph_from_documents(docs: Sequence[Document]) -> KnowledgeGraph:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¾¤ã‹ã‚‰Knowledge Graphã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    kg = KnowledgeGraph()
    for doc in docs:
        kg.nodes.append(
            Node(
                type=NodeType.DOCUMENT,
                properties={
                    "page_content": doc.page_content,
                    "document_metadata": doc.metadata,
                },
            )
        )
    return kg


def apply_default_transforms(
    kg: KnowledgeGraph,
    llm: LangchainLLMWrapper,
    headline_max: int = None,
    splitter_max_tokens: int = None,
) -> None:
    """è¦‹å‡ºã—æŠ½å‡ºãƒ»åˆ†å‰²ãƒ»ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºã®å¤‰æ›ã‚’é©ç”¨ã™ã‚‹ã€‚"""
    if headline_max is None:
        headline_max = config.CREATE_TESTSET_HEADLINE_MAX
    if splitter_max_tokens is None:
        splitter_max_tokens = config.CREATE_TESTSET_SPLITTER_MAX_TOKENS
    transforms = [
        HeadlinesExtractor(llm=llm, max_num=headline_max),
        HeadlineSplitter(max_tokens=splitter_max_tokens),
        KeyphrasesExtractor(llm=llm),
    ]
    apply_transforms(kg, transforms=transforms)


def build_query_distribution(
    llm: LangchainLLMWrapper,
    headline_weight: float,
    keyphrase_weight: float,
) -> List[Tuple[SingleHopSpecificQuerySynthesizer, float]]:
    """ã‚·ãƒ³ã‚°ãƒ«ãƒ›ãƒƒãƒ—ã‚¯ã‚¨ãƒªã®é‡ã¿ä»˜ãåˆ†å¸ƒã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    return [
        (SingleHopSpecificQuerySynthesizer(llm=llm, property_name="headlines"), headline_weight),
        (SingleHopSpecificQuerySynthesizer(llm=llm, property_name="keyphrases"), keyphrase_weight),
    ]


def generate_testset(
    kg: KnowledgeGraph,
    llm: LangchainLLMWrapper,
    embeddings: OpenAIEmbeddings,
    personas: Sequence[Persona],
    query_distribution: Iterable[Tuple[SingleHopSpecificQuerySynthesizer, float]],
    testset_size: int,
):
    """æŒ‡å®šè¨­å®šã§ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    generator = TestsetGenerator(
        llm=llm,
        embedding_model=embeddings,
        knowledge_graph=kg,
        persona_list=list(personas),
    )
    return generator.generate(testset_size=testset_size, query_distribution=list(query_distribution))


def save_testset_to_csv(testset, output_path: Path) -> None:
    """ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’CSVã§ä¿å­˜ã™ã‚‹ã€‚"""
    df = testset.to_pandas()
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(
        output_path,
        index=False,
        encoding="utf-8-sig", # BOMã¤ãã§å‡ºåŠ›ã—ã€Excelã§æ–‡å­—åŒ–ã‘ã—ãªã„
        quoting=csv.QUOTE_ALL,
        doublequote=True,
    )
    print(f"\nğŸ’¾ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")


def main() -> None:
    # ===== ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰è¨­å®š ====================================================
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚") 
    
    # llm_model_name = "gpt-5" # 2å€‹ä½œæˆã§1$ 20å€‹ä½œæˆã§2$?
    llm_model_name = config.CREATE_TESTSET_LLM_MODEL_NAME
    embedding_model_name = config.CREATE_TESTSET_EMBEDDING_MODEL_NAME

    # gpt-5-miniã¯temperature=1ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
    generator_llm = LangchainLLMWrapper(
        ChatOpenAI(
        model=llm_model_name,
        temperature=config.CREATE_TESTSET_LLM_TEMPERATURE,
        ),
        bypass_temperature=True  # temperatureã‚’ãƒ¦ãƒ¼ã‚¶ã§å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹è¨­å®š
    )

    openai_client = openai.OpenAI(api_key=openai_api_key)
    generator_embeddings = OpenAIEmbeddings(client=openai_client, model=embedding_model_name)

    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    output_dir = project_root / config.CREATE_TESTSET_OUTPUT_DIR
    output_csv_path = output_dir / f"testset_{timestamp}.csv"

    testset_size = config.CREATE_TESTSET_SIZE
    headline_weight = config.CREATE_TESTSET_HEADLINE_WEIGHT
    keyphrase_weight = config.CREATE_TESTSET_KEYPHRASE_WEIGHT

    chunk_mds_glob_pattern = config.CREATE_TESTSET_CHUNK_MDS_GLOB  # testsetä½œæˆç”¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
    print("\nå–å¾—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ:")
    for file in glob.glob(chunk_mds_glob_pattern):
        print(file)

    personas = [
        Persona(
            name="é‡‘èè¡Œæ”¿ã‚¢ãƒŠãƒªã‚¹ãƒˆ",
            role_description="é‡‘èåºã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ‹…å½“ã¨ã—ã¦ã€ä¿é™ºä¼šç¤¾ã®ç¤¾ä¼šçš„å½¹å‰²ã‚„è«¸èª²é¡Œã¸ã®å¯¾å¿œçŠ¶æ³ã‚’æ´å¯Ÿã—ã€æ”¿ç­–åˆ¤æ–­ã«æ´»ã‹ã™ãŸã‚ã®æƒ…å ±ã‚’å¿…è¦ã¨ã™ã‚‹ã€‚",
        ),
        Persona(
            name="ä¿é™ºä¼šç¤¾çµŒå–¶ä¼ç”»æ‹…å½“",
            role_description="å°‘å­é«˜é½¢åŒ–ã‚„ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã‚’è¸ã¾ãˆãŸæŒç¶šå¯èƒ½ãªãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã‚’æ¤œè¨ã—ã€å–¶æ¥­ãƒãƒ£ãƒãƒ«ã‚„å•†å“é–‹ç™ºã®æ–¹å‘æ€§ã‚’æ¢ã£ã¦ã„ã‚‹ã€‚",
        ),
        Persona(
            name="ãƒªã‚¹ã‚¯ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆè²¬ä»»è€…",
            role_description="è‡ªç„¶ç½å®³ãƒªã‚¹ã‚¯ã‚„å†ä¿é™ºæ–™ç‡ã®ä¸Šæ˜‡ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€ç•°å¸¸å±é™ºæº–å‚™é‡‘ã®æ´»ç”¨ã‚„æ°´ç½æ–™ç‡ã®ç´°åˆ†åŒ–ãªã©ã®å®Ÿå‹™çš„ãªæ–½ç­–ã‚’æ¯”è¼ƒæ¤œè¨ã—ã¦ã„ã‚‹ã€‚",
        ),
        Persona(
            name="ç¥æˆ¸å¸‚è¦³å…‰æ”¿ç­–æ‹…å½“",
            role_description="ä»¤å’Œ5å¹´åº¦ç¥æˆ¸å¸‚è¦³å…‰å‹•å‘èª¿æŸ»ã‚’è¸ã¾ãˆã€å¥³æ€§æ¯”ç‡ã‚„60æ­³ä»¥ä¸Šæ¥è¨ªè€…ãŒå¤šã„åœ°åŒºã¸ã®æ–½ç­–æ¤œè¨ã®ãŸã‚ã€åœ°åŒºåˆ¥å±æ€§ãƒ‡ãƒ¼ã‚¿ã‚’ç²¾æŸ»ã—ã¦è¦³å…‰è¡Œæ”¿ã®æ”¹å–„æ¡ˆã‚’å°ããŸã„ã€‚",
        ),
        Persona(
            name="ãƒãƒ¼ãƒãƒ¼ãƒ©ãƒ³ãƒ‰é›†å®¢ãƒãƒ¼ã‚±ã‚¿ãƒ¼",
            role_description="ç¥æˆ¸æ¸¯ã‚¨ãƒªã‚¢ã®æƒ…å ±åé›†ãƒãƒ£ãƒãƒ«ã®å·®åˆ†ï¼ˆæ—…è¡Œå‰å¾Œã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆåˆ©ç”¨å‰²åˆãªã©ï¼‰ã‚’æŠŠæ¡ã—ã€å†æ¥è¨ªæ„å‘ã¨ã‚¤ãƒ™ãƒ³ãƒˆæ–½ç­–ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®ç¤ºå”†ã‚’æ±‚ã‚ã¦ã„ã‚‹ã€‚",
        ),
        Persona(
            name="ç¥æˆ¸å¸‚äº¤é€šæ”¿ç­–ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼",
            role_description="åœ°åŒºã”ã¨ã®ä¸»ãªäº¤é€šæ‰‹æ®µï¼ˆè¥¿åŒ—ç¥ã§ã®è»Šåˆ©ç”¨ã‚„åŒ—é‡ã§ã®æ–°å¹¹ç·šå‰²åˆãªã©ï¼‰ã‚’åˆ†æã—ã€è¦³å…‰å®¢ã®ç§»å‹•å‹•ç·šã«åˆã‚ã›ãŸäº¤é€šã‚¤ãƒ³ãƒ•ãƒ©ãƒ»å‘¨éŠæ–½ç­–ã‚’è¨­è¨ˆã—ãŸã„ã€‚",
        ),
    ]

    # ===========================================================================

    chunk_glob_absolute = project_root / Path(chunk_mds_glob_pattern)
    matched_markdown = sorted(Path(p) for p in glob.glob(str(chunk_glob_absolute)))
    if not matched_markdown:
        raise FileNotFoundError(
            f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{chunk_glob_absolute}' ã«ä¸€è‡´ã™ã‚‹MarkdownãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            " `chunk_mds_glob_pattern` ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ `create_mds_from_chunks.py` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    loader = DirectoryLoader(str(project_root), glob=chunk_mds_glob_pattern)
    docs = loader.load()
    kg = build_knowledge_graph_from_documents(docs)
    apply_default_transforms(kg, generator_llm)

    # query_distributionç”Ÿæˆ
    query_distribution = build_query_distribution(
        llm=generator_llm,
        headline_weight=headline_weight,
        keyphrase_weight=keyphrase_weight,
    )

    # testsetç”Ÿæˆ
    testset = generate_testset(
        kg=kg,
        llm=generator_llm,
        embeddings=generator_embeddings,
        personas=personas,
        query_distribution=query_distribution,
        testset_size=testset_size,
    )

    # testsetä¿å­˜
    save_testset_to_csv(testset=testset, output_path=output_csv_path)


if __name__ == "__main__":
    main()
